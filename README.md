# Finetuning-LLM-using-QLoRA

![image](https://github.com/newacronym/Finetuning-LLM-using-QLoRA/assets/51745787/d9ad03a3-d210-4e24-93ea-645dd1e9f694)

## Dataset 
DialogSum from HuggingFace. It is an extensive dialogue summarization dataset, featuring 13,460 dialogues.
<https://huggingface.co/datasets/neil-code/dialogsum-test>

## PreTrained Model
Phi-2, a small Language Model (SLM) developed by Microsoft, with 2.7 billion parameters.

## Quantization
QLORA -> Quantizead Low rank adoptation

## Fine-tuning Method
PEFT -> Parameter Efficient Fine Tuning

## Results Obtained using ROGUE Metrics
![image](https://github.com/newacronym/Finetuning-LLM-using-QLoRA/assets/51745787/411c6dce-f9f9-4762-ada4-e5db7d873640)
